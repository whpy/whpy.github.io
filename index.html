<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-quick-start-for-DDPM-derivation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/24/quick-start-for-DDPM-derivation/" class="article-date">
  <time class="dt-published" datetime="2025-07-24T13:47:16.000Z" itemprop="datePublished">2025-07-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/24/quick-start-for-DDPM-derivation/">quick start for DDPM derivation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        

	<div class="row">
    <embed src="/pdf/quick_start_for_the_theory_of_diffusion_model.pdf" width="100%" height="550" type="application/pdf">
	</div>



      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/24/quick-start-for-DDPM-derivation/" data-id="cmdhg9thz00005ctp60iya2e1" data-title="quick start for DDPM derivation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-new-post" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/24/new-post/" class="article-date">
  <time class="dt-published" datetime="2025-07-24T10:52:32.000Z" itemprop="datePublished">2025-07-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/24/new-post/">new post</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/24/new-post/" data-id="cmdh9zp9k0000t4tpcj990g8z" data-title="new post" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-phd-ss" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/11/phd-ss/" class="article-date">
  <time class="dt-published" datetime="2024-05-11T02:41:02.000Z" itemprop="datePublished">2024-05-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/05/11/phd-ss/">phd_ss</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>This world does not require yet another mediocre PhD, but rather, an endeavor that serves 5% of humanity. Initiatives dedicated to narrowing the gap between intellects hold enduring value.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/11/phd-ss/" data-id="clw1iiiuj0001woszcbvrb9mg" data-title="phd_ss" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/saysay/" rel="tag">saysay</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-thinking-of-software-design-in-AI-era" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/28/thinking-of-software-design-in-AI-era/" class="article-date">
  <time class="dt-published" datetime="2024-03-28T14:25:09.000Z" itemprop="datePublished">2024-03-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/28/thinking-of-software-design-in-AI-era/">thinking of software design in AI era</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Some thinking about program in AI era.</p>
<h2 id="A-bitten-lesson-author-Rich-Sutton"><a href="#A-bitten-lesson-author-Rich-Sutton" class="headerlink" title="A bitten lesson(author: Rich Sutton)"></a>A bitten lesson(author: Rich Sutton)</h2><p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore’s law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to. Time spent on one is time not spent on the other. There are psychological commitments to investment in one approach or the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.  There were many examples of AI researchers’ belated learning of this bitter lesson, and it is instructive to review some of the most prominent.</p>
<p>In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that &#96;&#96;brute force” search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not.</p>
<p>A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. Also important was the use of learning by self play to learn a value function (as it was in many other games and even in chess, although learning did not play a big role in the 1997 program that first beat a world champion). Learning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear. Search and learning are the two most important classes of techniques for utilizing massive amounts of computation in AI research. In computer Go, as in computer chess, researchers’ initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning.</p>
<p>In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. Entrants included a host of special methods that took advantage of human knowledge—knowledge of words, of phonemes, of the human vocal tract, etc. On the other side were newer methods that were more statistical in nature and did much more computation, based on hidden Markov models (HMMs). Again, the statistical methods won out over the human-knowledge-based methods. This led to a major change in all of natural language processing, gradually over decades, where statistics and computation came to dominate the field. The recent rise of deep learning in speech recognition is the most recent step in this consistent direction. Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems. As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked—they tried to put that knowledge in their systems—but it proved ultimately counterproductive, and a colossal waste of researcher’s time, when, through Moore’s law, massive computation became available and a means was found to put it to good use.</p>
<p>In computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better.</p>
<p>This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that </p>
<ol>
<li>AI researchers have often tried to build knowledge into their agents, </li>
<li>this always helps in the short term, and is personally satisfying to the researcher, but </li>
<li>in the long run it plateaus and even inhibits further progress, and </li>
<li>breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.</li>
</ol>
<p>One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.</p>
<p>The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.</p>
<h2 id="Thinking"><a href="#Thinking" class="headerlink" title="Thinking"></a>Thinking</h2><p>In the bitter lesson above, prof. sutton emphasized the importance of computation: “…  general methods that leverage computation are ultimately the most effective”, “…but the only thing that matters in the long run is the leveraging of computation…”, also pointed out (some negative) effect of human experience to certain task: “… but all those efforts proved irrelevant, or worse, once search was applied effectively at scale…”</p>
<ol>
<li>For AI research, we should more focus on how to better utilize the computation resource.<br>Someone once summarized that principles of OpenAI lead to their success:</li>
<li></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/28/thinking-of-software-design-in-AI-era/" data-id="clw1iiiut0003wosze6rm6xlb" data-title="thinking of software design in AI era" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/misc-reprint/" rel="tag">misc, reprint</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-code-generator-and-AI" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/15/code-generator-and-AI/" class="article-date">
  <time class="dt-published" datetime="2024-03-15T01:48:31.000Z" itemprop="datePublished">2024-03-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/15/code-generator-and-AI/">code generator and AI</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Feature-of-code-generator"><a href="#Feature-of-code-generator" class="headerlink" title="Feature of code generator"></a>Feature of code generator</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>A record for some thinking about code generator</p>
<h2 id="Feature-of-code-generation"><a href="#Feature-of-code-generation" class="headerlink" title="Feature of code generation"></a>Feature of code generation</h2><p>It is natural for one to think that code generation is a subset of text generation, which implies we only need to focus on developing much more powerful LLM. But I think that this is not a simple brutal problem, here we first need to carefully consider the feature of code generation first.</p>
<p>First, the state and action space of a certain language is limited, actually not so much compared with natural language. And their usage and combination are even fixed for syntax.</p>
<p>Secondly, the coding is a totally rule complete game, not like most of the language task like translation but rather like go and chess. And compared with chess even we have a very powerful umpire, compilers, which could clearly tell you what your problem is. But the only difference is that coding is more like a open world game with no clear target.</p>
<p>Third, the local complexity of code is much lower than chess. For practical work, most of the codes are actually copied from existing code. It represents that fine-tune is the essence for over 60% percent of coding.</p>
<h2 id="Imagination"><a href="#Imagination" class="headerlink" title="Imagination"></a>Imagination</h2><p>based on the points above, we would find that essence of code generator could be summarized in two points:</p>
<ol>
<li>translate the requirements into flow graph;</li>
<li>generate the code based on the flow graph;<br>It is required that the code graph should be very accurate and logical, it should totally obey rule, which is quite different from  simple text generation.</li>
</ol>
<p>For the first part, I think we could achieve it by LLM with some self adapted mechanism. The mainly responsibility of this part is to set a target for the open-world game(the flow graph).</p>
<p>For the second part, we may could mimic the robotic arm, we could introduce diffusion strategies to make a code generator. The reason we introduce diffusion model is that it could well handle and fine-tune existing results to fulfill the requirements. And it show great performance on robotic movement planning. Consider the similarity of the essence of coding and robotic arm, it is worthy to try this strategy.</p>
<p>Furthermore, we are actually mimicing how alphago develop. As we note the similarity between coding and go.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/15/code-generator-and-AI/" data-id="clw1iiiu70000wosz0ko08dtz" data-title="code generator and AI" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/misc-art-imagination/" rel="tag">misc, art imagination</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-positional-encoder-is-all-you-need" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/01/positional-encoder-is-all-you-need/" class="article-date">
  <time class="dt-published" datetime="2024-03-01T08:21:18.000Z" itemprop="datePublished">2024-03-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/03/01/positional-encoder-is-all-you-need/">positional encoder is all you need</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Apetizer"><a href="#Apetizer" class="headerlink" title="Apetizer"></a>Apetizer</h2><p>Obviously, the inept title intends to pay tribute to <em>“attention is all you need”</em>. Google researchers boldly introduce attention mechanism as the absolute protagonist in word predicting, and they achieve incredible outstanding results. Then OpenAI totally implements a simplified version of transformer and develop the ability to utmost. GPT-series validate that something definitely different emerge in this framework of genius.  </p>
<h2 id="Soup"><a href="#Soup" class="headerlink" title="Soup"></a>Soup</h2><p>Due to the permutation-invariant of words in new mechanism, they naturally introduce positional encoders to identify the relative or absolute position of each word. We all know that order would largely affect meaning of sentence, “I love you” and “you love me” is totally different. So the additional information they introduce seems intuitive. </p>
<p>But we could imagine such things, though interchange of some key words would definitely change the whole sentence, it is an ordinary to come across such wrong sentences in our daily lives, but we could still identify what the author (probably) want to convey. Why? because the context, just like we could not identify the direction of time arrow by trajectory of single particle, but the problem would be trivial when we observe a group of particles. All the sentences we met actually are embedded into certain context, explicitly(context) or implicitly(common sense). The analogy aims to express that position encoding(PE) may be an redundant even convey unnecessary bias to the model. Additonally, in my opinion, when GPT read all the text in the world, it actually form his own common sense. </p>
<p>Though the inference above is novel and reasonable. But it is highly probably wrong. All the existing successful model verify that PE plays an essential role in prediction. </p>
<h2 id="Side-dish"><a href="#Side-dish" class="headerlink" title="Side dish"></a>Side dish</h2><p>So, we may could think conversely, although our inference contradicts to the experiment results. We could interpret it from a new perspective. Here we propose an assumption: word created by human naturally carrying the information of local position. PE actually introduce a weak instruction to order the sentences globally, to some extents, will. We could unstrictly verify this assumption by the sex trap jokes.</p>
<h2 id="main-dish"><a href="#main-dish" class="headerlink" title="main dish"></a>main dish</h2><p>Further, we boldly propose such things: order is the ultimate general goal for language. The goal of all kinds of expressions, no matter painting, text, song, is to emphasize something(that is the origin of attention mechanism), that implies kind of order. But if we try to equal order and attention, we would find an important difference, sparsity.<br>For attention mechanism, we could expects that the thing represents attention would definitely be sparse, but we seems could not expect anything could intuitively guarantee same property occurs in order representation. But inspired by the paper^[<a target="_blank" rel="noopener" href="https://www-users.cse.umn.edu/~odlyzko/doc/arch/random.shuffles.pdf]">https://www-users.cse.umn.edu/~odlyzko/doc/arch/random.shuffles.pdf]</a>, we envision such association: the sparsity of order lies in the linear representation of group. It could be observed that there exists plenty of zeroes in the most trivial construction of group representation. And just as words, group representation could be both object and functor(could affect and be affected, mimic the concept in functional programme), so their must exists some deep association between word representation and group representation. RoPe encoding partly supports this assumption, as it actually is a 2-order representation of certain group.</p>
<p>Furthermore, what is the novel point of our idea, the idea is we rely on less bias and experience, in the other words more general, to analyze the phenomenon of language and expression(image, voice). It is always meaningful to try to associate CV with LM, because they seems so different but the basic tools are so consistent. Moreover, the present LLMs still starts from language created by human, we may could induce the model to create their own language.</p>
<h2 id="Salad"><a href="#Salad" class="headerlink" title="Salad"></a>Salad</h2><p>For feasibility, we could verify this assumption from dynamic systems. As for any dynamic system results, they are totally different from words and images. Density of information of certain numerical result is far less than word, because word is very “clear”; but it is also more clear than image-like signal, as it ultimately could be expressed by mathematical equations. It seems lie on a threshold of information carrier. More concisely, the information of common dynamic systems are easily hign in dimensions, but the existence of governing equation guarantee there exists way to explore some method to compress the result, furthermore, express local information by another local information(bootstrap).  </p>
<p>Maybe, we could expect one day we find a good algorithm to let a dynamic system find their own language and with which elaborate their profound nature to us.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/03/01/positional-encoder-is-all-you-need/" data-id="clt8g88nr0000posza6khe0e1" data-title="positional encoder is all you need" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/crazy-Eureka/" rel="tag">crazy, Eureka</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-a-kind-of-generalized-method-to-analyze-time-sequence-of-turbulence" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/29/a-kind-of-generalized-method-to-analyze-time-sequence-of-turbulence/" class="article-date">
  <time class="dt-published" datetime="2024-02-29T07:26:24.000Z" itemprop="datePublished">2024-02-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/02/29/a-kind-of-generalized-method-to-analyze-time-sequence-of-turbulence/">a kind of generalized method to analyze time sequence</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Track"><a href="#Track" class="headerlink" title="Track"></a>Track</h2><p>Based on an interesting observation, we propose a novel way to analyze evolution of time sequence. We first conduct a thought experiment: if we give a chess player a series snapshots of the same endgame in random order, whether it is possible for him to sort the sequence correctly? I have tested it with my intelligent neighbor(amature chinese chess player), he could solve this problem quickly. By interviewing him, he told me many tricks and points you need to focus on, which he had never summarized before(make sense).</p>
<p>So we may mimic such simple experiments, to investigate how much does the model understand the essence of the problem. We so far find similar work published in 2017^[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08496.pdf]">https://arxiv.org/pdf/2206.08496.pdf]</a> and related theory survey^[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.01285.pdf]">https://arxiv.org/pdf/1907.01285.pdf]</a>. In the CV work, authors claims that their model achieve SOTA on classification by introducing such unsupervised method to pretrain the network, model performs much better in comparation; In the latter paper, author propose the association between sorting and reinforcement learning.</p>
<p>It is interesting to note that to some distent, this problem implies an explicit solution to extract the characteristics of time sequence, temporal signal decomposition. When we transform the signal from time domain to frequency domain, we actually try to find out the periodic pattern of the signal sequences. We could expect that when all the people in a bar move strictly periodically, it is impossible to sort all the snapshots correctly. For human, we may could only sort the subsequence in one common period, it is understandable.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/29/a-kind-of-generalized-method-to-analyze-time-sequence-of-turbulence/" data-id="clt7b9u6l0000d8sz0xc7g0en" data-title="a kind of generalized method to analyze time sequence" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Eureka-fluid-computer/" rel="tag">Eureka, fluid computer</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-some-useful-tricks-in-persuading-others" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/29/some-useful-tricks-in-persuading-others/" class="article-date">
  <time class="dt-published" datetime="2024-02-29T05:30:55.000Z" itemprop="datePublished">2024-02-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/02/29/some-useful-tricks-in-persuading-others/">some useful tricks in persuading others</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Here are some tricks that may be useful for saling ideas. This blog provides a brief summary of them.</p>
<h2 id="1-Adequate-preparations-beforehead"><a href="#1-Adequate-preparations-beforehead" class="headerlink" title="1. Adequate preparations beforehead"></a>1. Adequate preparations beforehead</h2><p>Preparation contributes to over 60% of a successful persuasion. Preparation work could be divided into following two parts:</p>
<h3 id="1-1-300-convince-yourself"><a href="#1-1-300-convince-yourself" class="headerlink" title="1.1 300% convince yourself"></a>1.1 300% convince yourself</h3><p>First, try to fully convince yourself. Try to ask all the sharp questions you could imagine to yourself, following the approach of GAN. It is ok to find some points that you don’t have perfect solution to it, but at least proposing some approaches to transform the problem. The reason we use 300% is that for the whole idea you want to sale to others, no more than 30%(even 20%) of it will be discussed in the first talk(it would required several rounds of communication to bind us and customers together), but you need to explore deeper layers of value in advance. In the other words, if an idea could be dug out more than 3 layers of value, it is highly possible to be a good idea.</p>
<p>In addition, it is highly possible that person sitting opposite may not be ideal, so luck accounts for 20%.</p>
<h2 id="1-2-know-20-of-the-customer"><a href="#1-2-know-20-of-the-customer" class="headerlink" title="1.2 know 20% of the customer"></a>1.2 know 20% of the customer</h2><p>Know the research taste of customer, what are the customer interests or concern most, money, reputation, experience, interest. Adjust the emphasis for different object.</p>
<p>Know the unique reasoning logic of the customer, that is the handle we need to guide his&#x2F;her thought in the face-to-face communication.</p>
<p>Know his&#x2F;her experience, it is the point that we start the communication.</p>
<h2 id="2-guide-the-customer-to-follow-prepared-reasoning"><a href="#2-guide-the-customer-to-follow-prepared-reasoning" class="headerlink" title="2 guide the customer to follow prepared reasoning"></a>2 guide the customer to follow prepared reasoning</h2><p>the most ideal strategy is to let the customer reason the final answer by himself. Here are some tricks.</p>
<h3 id="2-1-split-the-big-picture-into-multiple-sub-goals"><a href="#2-1-split-the-big-picture-into-multiple-sub-goals" class="headerlink" title="2.1 split the big picture into multiple sub-goals"></a>2.1 split the big picture into multiple sub-goals</h3><p>We don’t directly depict the final answer to the customer, we guide him to find the final point step by step from ordinary to extraordinary. Here are some rough division:</p>
<h4 id="2-1-1-Introduction"><a href="#2-1-1-Introduction" class="headerlink" title="2.1.1 Introduction"></a>2.1.1 Introduction</h4><p> we start from some trivial cases with very a little bit novel blind points. We could repeat the definitions of fundamental concepts with customer to reach consensus, also generalize the definitions to approach the field of final point. Remember try to make the customer to reason the lemma by himself.</p>
<h4 id="2-1-2-series-of-feasible-sub-tasks"><a href="#2-1-2-series-of-feasible-sub-tasks" class="headerlink" title="2.1.2 series of feasible sub-tasks"></a>2.1.2 series of feasible sub-tasks</h4><p> When we let the customer get some feeling about the novel point or interesting point, we will immediately tell the customer how to achieve it step. That is the preparation work you need to do in preparation stage. </p>
<p> Here is one of the good narrative approach:</p>
<ol>
<li><p>tell the customer that we need to do several verification tests to verify the feasibility of the final idea, and when we finish the tests we could get a weakened version of the final goal. Also emphasize the minimum return no matter how the results of verification tests are.</p>
</li>
<li><p>Analyze to the customer that the final goal could be achieve by linearly enhance the original weakened version (kind of scale rule proposed by Ilya ).</p>
</li>
</ol>
<h3 id="3-profit-division"><a href="#3-profit-division" class="headerlink" title="3 profit division"></a>3 profit division</h3><p>For the core technology partner, we do not propose how to divide the profit actively. For a meaningful idea, it is impossible to forecast the total final return of it. If we could be convinced in the preparation stage, we should definitely believe that the project is invaluable.</p>
<p>For the investigator, we should try to propose a feasible business mode to them. No matter how, and the technology is not they want to know, they only concern about profit and ROI, they want to make sure that they investigate the money on a not bad direction.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/29/some-useful-tricks-in-persuading-others/" data-id="clt7b9u720001d8sz0l3p4g37" data-title="some useful tricks in persuading others" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/misc-tricks/" rel="tag">misc, tricks</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-nektar-quickstart" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/13/nektar-quickstart/" class="article-date">
  <time class="dt-published" datetime="2024-02-12T16:30:16.000Z" itemprop="datePublished">2024-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/02/13/nektar-quickstart/">nektar quickstart</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/13/nektar-quickstart/" data-id="clsjvse5k0000d0sz00neckkh" data-title="nektar quickstart" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/13/test/" class="article-date">
  <time class="dt-published" datetime="2024-02-12T16:02:16.000Z" itemprop="datePublished">2024-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/02/13/test/">test</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>this is a simple file just for test.</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>successfully generate a blog in markdown format; Successfully synchronize the blog contents with the remote repository; discard the unused blog files.</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>All the following commands are run on the git bash installed on windows:</p>
<ol>
<li><p><strong>New a file</strong> </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;test&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Generate related files</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Synchronize(deploy) the files to remote github repo</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Delete Unneeded blog</strong><br>Firstly, delete the markdown files in the <code>source/</code>. Then, we run:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>create folder for draft</strong></p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo n draft &quot;$(name_of_draft)&quot;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/13/test/" data-id="clsj4lfzo0000poszep2ch2ig" data-title="test" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Eureka-fluid-computer/" rel="tag">Eureka, fluid computer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crazy-Eureka/" rel="tag">crazy, Eureka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/misc-art-imagination/" rel="tag">misc, art imagination</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/misc-reprint/" rel="tag">misc, reprint</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/misc-tricks/" rel="tag">misc, tricks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/saysay/" rel="tag">saysay</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Eureka-fluid-computer/" style="font-size: 10px;">Eureka, fluid computer</a> <a href="/tags/crazy-Eureka/" style="font-size: 10px;">crazy, Eureka</a> <a href="/tags/misc-art-imagination/" style="font-size: 10px;">misc, art imagination</a> <a href="/tags/misc-reprint/" style="font-size: 10px;">misc, reprint</a> <a href="/tags/misc-tricks/" style="font-size: 10px;">misc, tricks</a> <a href="/tags/saysay/" style="font-size: 10px;">saysay</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/24/quick-start-for-DDPM-derivation/">quick start for DDPM derivation</a>
          </li>
        
          <li>
            <a href="/2025/07/24/new-post/">new post</a>
          </li>
        
          <li>
            <a href="/2024/05/11/phd-ss/">phd_ss</a>
          </li>
        
          <li>
            <a href="/2024/03/28/thinking-of-software-design-in-AI-era/">thinking of software design in AI era</a>
          </li>
        
          <li>
            <a href="/2024/03/15/code-generator-and-AI/">code generator and AI</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>